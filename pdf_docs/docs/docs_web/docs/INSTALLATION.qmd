---
title: "Installation Guide"
author: "Sofia Billger Bergstr√∂m"
format: html
---

# Installation

This guide provides detailed instructions for setting up `code-agent` and its dependencies on your local machine.

## 1. Prerequisites

Before you begin, ensure you have the following installed:

- **Python**: Version 3.10 or newer.
- **Git**: For cloning the repository.
- **Ollama**: The agent relies on [Ollama](https://ollama.com/) to run local language models. Please follow their
  instructions to install it on your system.

After installing Ollama, you must pull a model for the agent to use. We recommend starting with `llama3` or a similar
instruction-tuned model.

```bash
ollama pull llama3
```

## 2. Cloning the Repository

Get the source code by cloning the repository from GitHub.

```bash
git clone https://github.com/your-username/code_agent.git
cd code_agent
```

## 3. Setting up the Environment and Installing Dependencies

We strongly recommend using `uv` for fast and reliable dependency management.

### Using `uv` (Recommended)

1. **Create and activate a virtual environment**:
   ```bash
   uv venv
   source .venv/bin/activate
   ```

2. **Install the package**: To install the project with all core and development dependencies, run:
   ```bash
   uv pip install -e .[dev]
   ```
   This installs the package in "editable" mode, which means that changes you make to the source code will be
   immediately available without needing to reinstall.

### Using `pip`

If you prefer to use `pip`, you can follow these steps:

1. **Create and activate a virtual environment**:
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```

2. **Install the package**:
   ```bash
   pip install -e .[dev]
   ```

## 4. Configuration

The agent's behavior is controlled by the `code_agent/config/llm_config.json` file. Before running the agent for the
first time, ensure the `ollama_model` specified in this file matches the model you pulled with Ollama.

**Example `llm_config.json`:**
```json
{
  "ollama_model": "llama3",
  "temperature": 0.7,
  "ollama_host": "http://localhost:11434"
}
```

## 5. Verifying the Installation

You can verify that the installation was successful by running the interactive chat agent.

```bash
code-agent chat
```

If the agent starts up and you see the "Agent ready!" message, your installation is complete.
